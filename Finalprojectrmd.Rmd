---
title: "Final Project"
author: "John Compton, David Counter, Bhanu Kappala, and Ethan Surdykowski"
date: "December 17, 2019"
output: html_document
---


```{r, setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", 
                      cache = TRUE, autodep = TRUE)
```

```{r, load-packages, include = FALSE, message = FALSE}
library("tidyverse")
library("dplyr")
library("rsample")
library("caret")
library("kableExtra")
library("doParallel")
library("microbenchmark")
library(foreach)
library(parallel)
library(gridExtra)



```

```{r, setup-parallel}
registerDoParallel(cores = 4)
```

```{r data merge, eval=FALSE}

train <- read.csv("train.csv")
weather <- read.csv("weather.csv")
test<- read.csv("test.csv")
key <- read.csv("key.csv")
train$date <- as.Date(train$date, "%Y-%m-%d")
weather$date <- as.Date(weather$date, "%Y-%m-%d")
test$date <- as.Date(test$date, "%Y-%m-%d")


keymerge <- rep(1,4617600)


#merges weather with train based on key
  mcmapply( FUN = for(i in 1:4617600){
    keymerge[i] = key$station_nbr[train$store_nbr[i]]
    if(i %% 10000 == 0){
      print(i)
    }
  }
)

train<-cbind("station_nbr" = keymerge, train)
mergedata <- merge(train, weather, by = c("date", "station_nbr"))
#orders data based on original format
mergedata <- mergedata[with(mergedata, order(mergedata$date, mergedata$store_nbr, mergedata$item_nbr)),]
write.csv(mergedata, "mergedata.csv")

#removes all rows with unit = 0
cleandata <- subset(mergedata, !mergedata$units == 0)
write.csv(cleandata, "cleandata.csv")



#merges test data with weather based on key
test <- read.csv("test.csv")
keytest <- rep(0, nrow(test))
#create key
  mcmapply( FUN = for(i in 1:nrow(test)){
    keytest[i] = key$station_nbr[test$store_nbr[i]]
    if(i %% 10000 == 0){
      print(i)
    }
  }
)
newtest <- cbind("station_nbr" = keytest, test)


#merges test data with merge data to create a from for prediction
testtest <- merge(newtest, weather,  by = c("date", "station_nbr"))
drops <- c("X.1", "X")
#merges data based on standard format
testtest <- testtest[ , !names(testtest) %in% drops]
testtest <- testtest[with(testtest, order(testtest$date, testtest$store_nbr, testtest$item_nbr)),]

write.csv(testtest, "testy.csv")


```



```{r data load}
####RUN AT STARTUP
#initialized data
mergedata <- read.csv("mergedata.csv")
sample <- read.csv("sampleSubmission.csv")
testy <- read.csv("testy.csv")
cleandata <- read.csv("cleandata.csv")
drops <- c("X", "key", "codesum", "station_nbr")
mergedata <- mergedata[ , !names(mergedata) %in% drops]
testy <- testy[ , !names(testy) %in% drops]







#changes factor levels to num
mergedata$date <- as.Date(mergedata$date, "%Y-%m-%d")
mergedata$tmax <- as.numeric(levels(mergedata$tmax))[mergedata$tmax]
mergedata$tmin <- as.numeric(levels(mergedata$tmin))[mergedata$tmin]
mergedata$tavg <- as.numeric(levels(mergedata$tavg))[mergedata$tavg]
mergedata$depart <- as.numeric(levels(mergedata$depart))[mergedata$depart]
mergedata$dewpoint <- as.numeric(levels(mergedata$dewpoint))[mergedata$dewpoint]
mergedata$wetbulb <- as.numeric(levels(mergedata$wetbulb))[mergedata$wetbulb]
mergedata$heat <- as.numeric(levels(mergedata$heat))[mergedata$heat]
mergedata$cool <- as.numeric(levels(mergedata$cool))[mergedata$cool]
mergedata$sunrise <- as.numeric(levels(mergedata$sunrise))[mergedata$sunrise]
mergedata$sunset <- as.numeric(levels(mergedata$sunset))[mergedata$sunset]
mergedata$snowfall <- as.numeric(levels(mergedata$snowfall))[mergedata$snowfall]
mergedata$preciptotal <- as.numeric(levels(mergedata$preciptotal))[mergedata$preciptotal]
mergedata$stnpressure <- as.numeric(levels(mergedata$stnpressure))[mergedata$stnpressure]
mergedata$sealevel <- as.numeric(levels(mergedata$sealevel))[mergedata$sealevel]
mergedata$resultspeed <- as.numeric(levels(mergedata$resultspeed))[mergedata$resultspeed]
mergedata$resultdir <- as.numeric(levels(mergedata$resultdir))[mergedata$resultdir]
mergedata$avgspeed <- as.numeric(levels(mergedata$avgspeed))[mergedata$avgspeed]







testy$date <- as.Date(testy$date, "%Y-%m-%d")
testy$tmax <- as.numeric(levels(testy$tmax))[testy$tmax]
testy$tmin <- as.numeric(levels(testy$tmin))[testy$tmin]
testy$tavg <- as.numeric(levels(testy$tavg))[testy$tavg]
testy$depart <- as.numeric(levels(testy$depart))[testy$depart]
testy$dewpoint <- as.numeric(levels(testy$dewpoint))[testy$dewpoint]
testy$wetbulb <- as.numeric(levels(testy$wetbulb))[testy$wetbulb]
testy$heat <- as.numeric(levels(testy$heat))[testy$heat]
testy$cool <- as.numeric(levels(testy$cool))[testy$cool]
testy$sunrise <- as.numeric(levels(testy$sunrise))[testy$sunrise]
testy$sunset <- as.numeric(levels(testy$sunset))[testy$sunset]
testy$snowfall <- as.numeric(levels(testy$snowfall))[testy$snowfall]
testy$preciptotal <- as.numeric(levels(testy$preciptotal))[testy$preciptotal]
testy$stnpressure <- as.numeric(levels(testy$stnpressure))[testy$stnpressure]
testy$sealevel <- as.numeric(levels(testy$sealevel))[testy$sealevel]
testy$resultspeed <- as.numeric(levels(testy$resultspeed))[testy$resultspeed]
testy$resultdir <- as.numeric(levels(testy$resultdir))[testy$resultdir]
testy$avgspeed <- as.numeric(levels(testy$avgspeed))[testy$avgspeed]
```

```{r, ggplot-theme}
theme_set(theme_light())
```


```{r, load-data, message = FALSE, warning = FALSE}
library(readr)
cleandata <- read_csv("cleandata.csv")
cleandata$event[cleandata$preciptotal > 1 | cleandata$snowfall > 2] = "TRUE"
cleandata$event[is.na(cleandata$preciptotal) & (cleandata$snowfall > 2)] = "TRUE"
cleandata$event[is.na(cleandata$snowfall) & (cleandata$preciptotal > 1)] = "TRUE"
cleandata$event[!(cleandata$preciptotal > 1 | cleandata$snowfall > 2)] = "FALSE"
cleandata$event[is.na(cleandata$preciptotal) & is.na(cleandata$snowfall)] = "FALSE"
cleandata$event[is.na(cleandata$preciptotal) & (cleandata$snowfall <= 2)] = "FALSE"
cleandata$event[is.na(cleandata$snowfall) & (cleandata$preciptotal <= 1)] = "FALSE"
```

***

***

# Abstract

> Extreme weather events have large impacts on the surrounding communities, causing massive increases in demand for certain products. Walmart has a large inventory for these products and has aggregated data on weather events to be able to predict the amount of units of product necessary. A variety of statistical learning methods were explored and validated. 

***

# Introduction

	Weather has been noted to have an effect on retail sales creating fluctuations in demand because certain items are more useful during these weather events. [^1] This leads to a problem for producers with supply chain management and the so-called "bullwhip effect" [^2] where forecasts can lead to supply chain inefficiencies. Being able to make better predictions of the effect of the weather on sales will allow producers to choose a proper inventory and minimize the effect. 
	
	The models will be trained onto a training dataset where they will be selected via an estimation and validation dataset and then tested on the training dataset which will be evaluated through a kaggle score of the Root Mean Squared Logarithmic Error. The data will be trained on all potential features beginning with OLS. Model diagnostics and selection criteria will then be used to evaluate the linear model. We will also consider the performance of scaling models such as Ridge and Lasso Regressions. We will also examine quadratic relationships and interaction terms for explanatory power.
	
***

# Methods
	
## Data

  The following data was accessed from Kaggle, and collected by Walmart based on products whose sales may have been affected by weather events. [^3] The data was collected in 2014 and taken from 45 different stores with 20 different weather reporting stations across these areas. The data was merged from several sources, a training dataset which has the amount of units and weather data for the day. A weather event was defined as as any day in which more than an inch of rain or two inches of snow was observed.
  
  One of the problems with the data is that much of the weather data is missing values. Refer to table one in the appendix. Most of the weather data is missing values with sunrise and sunset missing 51.38% of their data. This suggests that we should drop most of the weather data, but use the dummy variable for whether there was a weather event.
  
```{r, exploratory-data-analysis,  message = FALSE, warning = FALSE}
p1 = cleandata %>% 
  ggplot(aes(x = units)) +
  geom_histogram(bins = 30) + 
  xlim(0, 200) +
  ylim(0, 15000) +
  ggtitle("Figure 1: Histogram of Units")

p2 = cleandata %>% 
  ggplot(aes(x = item_nbr, y = units)) + 
  geom_point(col = "orange") +
  ggtitle("Figure 2: Scatter Plot of Item Number vs Number of Units")

p3 = cleandata %>%
    ggplot(aes(x = event, fill = event)) +
    geom_bar() +
    ggtitle("Figure 3: Bar plot of Number of Weather Events")

gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

Beginning with some exploratory data analysis, we can see that the histogram of units is skewed left and appears non-normal, we will check our normality assumption on the regression model. In the plot of item number vs unit there appear to be two outliers which we will want to correct for as we move forward. Another item of note is that there is a large class imbalance with the number of weather events, we could consider doing SMOTE resampling as a method to correct for this. We will check the linear model for the cook's distance and ability to influence the regression under the modeling section.
	
## Modeling

```{r, ols, echo = TRUE}
model1 = lm(units ~ date + store_nbr + item_nbr + event, data = cleandata)
summary(model1)
```

	
#Linear Regression Model / Diagnostics 


```{r dataclean, eval=FALSE}
#applies data converting to remove as factor variables
#RUN AT START


mergedata$store_nbr <- as.factor(mergedata$store_nbr)
mergedata$item_nbr <- as.factor(mergedata$item_nbr)


testy$store_nbr <- as.factor(testy$store_nbr)
testy$item_nbr <- as.factor(testy$item_nbr)



```

```{r OLS}
SampleOLS <- sample


#OLS model
OLSModel <- lm(units ~ . , data = cleandata)

summary(OLSModel)




predictionOLS <- predict(OLSModel, newdata = testy)
predictionOLS[is.na(predictionOLS)] <- 0

SampleOLS$units <- predictionOLS

write.csv(x = SampleOLS, file =  "SampleOLS.csv", row.names = FALSE)


```

```{r AIC}
AICmerge <- cleandata

AICmerge <- na.omit(AICmerge)

AICModel <- lm(units ~ ., data = AICmerge)

SampleAIC <- sample
AICModel2 <- step(AICModel)

predictionAIC <- predict(AICModel2, newdata = testy)
predictionAIC[is.na(predictionAIC)] <- 0

SampleAIC$units <- predictionAIC

write.csv(x = SampleAIC, file =  "SampleAIC.csv", row.names = FALSE)



```

```{r}
mergedata = read.csv("mergedata.csv")
testy = read.csv("testy.csv")
cleandata = read.csv("cleandata.csv")
drops <- c("X", "key", "codesum", "station_nbr")
mergedata <- mergedata[ , !names(mergedata) %in% drops]
testy <- testy[ , !names(testy) %in% drops]
```

```{r}
cleandata$tmax = NULL
cleandata$tmin = NULL
cleandata$X = NULL
cleandata[cleandata$tavg == "M",]$tavg = NA
cleandata$tavg = as.numeric(cleandata$tavg)
testy$date = as.Date(testy$date)
cleandata$date = as.Date(cleandata$date)
testy$store_nbr = as.factor(testy$store_nbr)
testy$item_nbr = as.factor(testy$item_nbr)
cleandata$store_nbr <- as.factor(cleandata$store_nbr)
cleandata$item_nbr <- as.factor(cleandata$item_nbr)
grouped_data = cleandata[cleandata$units != 0,]
grouped_data$id = paste0(as.character(grouped_data$store_nbr), "_", as.character(grouped_data$item_nbr))
testy$id = paste0(as.character(testy$store_nbr), "_", as.character(testy$item_nbr))
```

```{r}
model = lm(units ~ date + store_nbr + item_nbr + dewpoint , data = grouped_data)
predictions = predict(model, testy)
testy$units = predictions
testy[!testy$id %in% grouped_data$id,]$units = 0
sample = read.csv("sampleSubmission.csv")
sample$units = testy$units
write.csv(sample, "testSubmission.csv", row.names = FALSE)
```











# Appendix

## Tables and Graphs

```{r, missing-data-table}
tibble(
  "Column" = colnames(cleandata),
  "Percentage of Missing Values" =  as.numeric(colMeans(is.na(cleandata)))
) %>%
  kable(caption = "Table 1: Percentage of Missing Values for Each Column of the Shopper Dataset", digits = 4) %>%
  kable_styling("striped", full_width = FALSE)
```



## Data Dictionary:

- `store_nbr` - an id representing one of the 45 stores
- `item_nbr` - an id representing one of the 111 products
- `units` - the quantity sold of an item on a given day
- `tmax` - the maximum temperature in Farenheit
- `tmin` - the minimum temperature in Farenheit
- `tavg` - the average temperature in Farenheit
- `depart` - the temperature departure from the normal
- `dewpoint` - the average dew point temperature, the atmospheric temperature below which water begins to condense
- `wetbulb` - the average wet bulb temperature, the temperature taken by a thermometer covered in a water soaked cloth
- `heat` - the difference between the temperature and 65 degrees Farenheit
- `cool` - the difference between the temperature and 65 degrees Farenheit
- `codesum` - a code identifying the type of weather
- `snowfall` - snow fall in inches
- `preciptotal` - total amount of precipitation
- `stnpressure` - average station pressure
- `sealevel` - average sea level pressure
- `resultspeed` - resultant wind speed
- `resultdir` - resultant wind direction
- `avgspeed` - average wind speed

For more information on the data, refer to the source on Kaggle.

[^1]: [Naples News](https://www.naplesnews.com/story/weather/hurricanes/2019/06/02/hurricane-2019-shoppers-take-advantage-hurricane-sales-tax-holiday/1320135001/)
[^2]: [Wikipedia: Bullwhip Effect](https://en.wikipedia.org/wiki/Bullwhip_effect)
[^3]: [Kaggle: Walmart Recruiting II](https://www.kaggle.com/c/walmart-recruiting-sales-in-stormy-weather/overview/evaluation)

















